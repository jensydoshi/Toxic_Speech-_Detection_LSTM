{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPr3neUXti6T"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx28O1_Yti6Z"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.callbacks import Callback\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XghKWBIRuLnn",
        "outputId": "217395d4-5387-4b9a-d836-02c40f1c56e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG0RawjBti6b"
      },
      "outputs": [],
      "source": [
        "train_data_file = \"drive/My Drive/Final year project/train.csv\"\n",
        "test_data_file = \"drive/My Drive/Final year project/test.csv\"\n",
        "submission_file = \"drive/My Drive/Final year project/test.csv\"\n",
        "\n",
        "train_data = pd.read_csv(train_data_file)\n",
        "test_data = pd.read_csv(test_data_file)\n",
        "submission_result = pd.read_csv(test_data_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL9sz5chti6c"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bQNYhsi0-qW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "max_len is the maximum length of a sequence of words (in terms of number of tokens) that will be input to a neural network model. Any sequence longer than this will be truncated, and any sequence shorter than this will be padded with zeros.\n",
        "\n",
        "embedding_dim is the dimensionality of the word embeddings that will be learned by the neural network. Word embeddings are vector representations of words that capture semantic and syntactic relationships between words.\n",
        "\n",
        "vocabulary_size is the maximum number of unique words that the model will consider. Any words outside of this vocabulary will be treated as out-of-vocabulary (OOV) words.\n",
        "\n",
        "num_tokens is equal to the vocabulary size plus one, since we need to include a token for OOV words. In this case, the OOV token is represented by 0"
      ],
      "metadata": {
        "id": "OUYr8xEk-s6H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf8YWmVgti6f"
      },
      "outputs": [],
      "source": [
        "# set up paramters\n",
        "max_len = 120\n",
        "embedding_dim = 300\n",
        "vocabulary_size = 20000 #35000\n",
        "num_tokens = vocabulary_size+1 #including 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBD7mhklti6g"
      },
      "outputs": [],
      "source": [
        "# preprocess comment texts\n",
        "def preprocess(corpus):\n",
        "\n",
        "# remove all non-English characters\n",
        "# and convert all letters to lower case\n",
        "    printable = set(string.printable)\n",
        "    corpus = ''.join(filter(lambda x: x in printable, corpus))\n",
        "    corpus = corpus.lower()\n",
        "\n",
        "# change contracted words into possible non-contracted form\n",
        "    # specific\n",
        "    corpus = re.sub(r\"won't\", \"will not\", corpus)\n",
        "    corpus = re.sub(r\"can\\'t\", \"can not\", corpus)\n",
        "    # or could be 'are not' etc\n",
        "    corpus = re.sub(r\"ain\\'t\",\"is not\", corpus)\n",
        "    corpus = re.sub(r\"shan\\'t\", \"shall not\", corpus)\n",
        "    corputs = re.sub(r\"let\\'s\", \"let us\", corpus)\n",
        "\n",
        "    # general\n",
        "    corpus = re.sub(r\"n\\'t\", \" not\", corpus)\n",
        "    corpus = re.sub(r\"\\'re\", \" are\", corpus)\n",
        "    corpus = re.sub(r\"\\'s\", \" is\", corpus)\n",
        "    # or could be \\'d --> had\n",
        "    corpus = re.sub(r\"\\'d\", \" would\", corpus)\n",
        "    corpus = re.sub(r\"\\'ll\", \" will\", corpus)\n",
        "    corpus = re.sub(r\"\\'t\", \" not\", corpus)\n",
        "    corpus = re.sub(r\"\\'ve\", \" have\", corpus)\n",
        "    corpus = re.sub(r\"\\'m\", \" am\", corpus)\n",
        "\n",
        "    # replace the rest \\' with ' '\n",
        "    corpus = re.sub(r\"\\'\", \" \", corpus)\n",
        "\n",
        "    correction_list = {\"youfuck\": \"you fuck\", \\\n",
        "                       \"fucksex\": \"fuck sex\",\\\n",
        "                       \"bitchbot\": \"bitch bot\",\\\n",
        "                       \"offfuck\": \"fuck off\",\\\n",
        "                       \"donkeysex\": \"donkey sex\",\\\n",
        "                      \"securityfuck\": \"security fuck\",\\\n",
        "                      \"ancestryfuck\": \"ancestry fuck\",\\\n",
        "                      \"turkeyfuck\": \"turkey fuck\",\\\n",
        "                      \"faggotgay\": \"faggot gay\",\\\n",
        "                       \"fuckbot\": \"fuck bot\",\\\n",
        "                       \"assfuckers\": \"ass fucker\",\\\n",
        "                       \"ckckck\": \"cock\",\\\n",
        "                       \"fuckfuck\": \"fuck\",\\\n",
        "                       \"lolol\": \"lol\",\\\n",
        "                       \"pussyfuck\": \"fuck\",\\\n",
        "                        \"gaygay\": \"gay\",\\\n",
        "                       \"haha\": \"ha\",\\\n",
        "                       \"sucksuck\": \"suck\"\n",
        "                      }\n",
        "    for old,new in correction_list.items():\n",
        "        corpus = corpus.replace(old,new)\n",
        "\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a Python function that performs text preprocessing on a given corpus. The function takes a single argument, corpus, which is a string of text. The text preprocessing steps that are performed by this function are described below:\n",
        "\n",
        "The first step is to remove all non-English characters from the corpus string, and convert all letters to lowercase. This is done using the filter() function and the lower() method of the string class.\n",
        "\n",
        "The next step is to replace contracted words with their possible non-contracted form. For example, \"won't\" is replaced with \"will not\", \"can't\" is replaced with \"can not\", and so on. This is done using regular expressions and the re.sub() function.\n",
        "\n",
        "The third step is to replace other common contractions with their expanded form. For example, \"n't\" is replaced with \"not\", \"'re\" is replaced with \"are\", \"'s\" is replaced with \"is\", and so on. This is also done using regular expressions and the re.sub() function.\n",
        "\n",
        "Finally, any remaining apostrophes (') are replaced with spaces. This is done using another call to re.sub().\n",
        "\n",
        "Overall, this function is useful for standardizing and cleaning up text data before further analysis or processing, and can be used as a pre-processing step in natural language processing pipelines.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0hvQLUh4BubF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MiW4miUc-vFS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5Fbuxpoti6j"
      },
      "outputs": [],
      "source": [
        "# tokenizer vocabulary_size words\n",
        "# we ignore all numbers\n",
        "tokenizer = Tokenizer(num_words = vocabulary_size+1,\\\n",
        "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n0123456789',\\\n",
        "                      lower=True, split=' ')\n",
        "\n",
        "# extract comment texts from train_data and test_data\n",
        "X_train_raw = train_data[\"comment_text\"]\n",
        "X_test_raw = test_data[\"comment_text\"]\n",
        "\n",
        "bad_comment_cat = ['toxic', 'severe_toxic', 'obscene', 'threat',\\\n",
        "       'insult', 'identity_hate']\n",
        "Y_train = train_data[bad_comment_cat]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a Python function that performs text preprocessing on a given corpus. The function takes a single argument, corpus, which is a string of text. The text preprocessing steps that are performed by this function are described below:\n",
        "\n",
        "The first step is to remove all non-English characters from the corpus string, and convert all letters to lowercase. This is done using the filter() function and the lower() method of the string class.\n",
        "\n",
        "The next step is to replace contracted words with their possible non-contracted form. For example, \"won't\" is replaced with \"will not\", \"can't\" is replaced with \"can not\", and so on. This is done using regular expressions and the re.sub() function.\n",
        "\n",
        "The third step is to replace other common contractions with their expanded form. For example, \"n't\" is replaced with \"not\", \"'re\" is replaced with \"are\", \"'s\" is replaced with \"is\", and so on. This is also done using regular expressions and the re.sub() function.\n",
        "\n",
        "Finally, any remaining apostrophes (') are replaced with spaces. This is done using another call to re.sub().\n",
        "\n",
        "Overall, this function is useful for standardizing and cleaning up text data before further analysis or processing, and can be used as a pre-processing step in natural language processing pipelines.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "jensy.doshi74@nmims.edu.in\n",
        "tokenizer = Tokenizer(num_words = vocabulary_size+1,\\\n",
        "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n0123456789',\\\n",
        "                      lower=True, split=' ')\n",
        "\n",
        "# extract comment texts from train_data and test_data\n",
        "X_train_raw = train_data[\"comment_text\"]\n",
        "X_test_raw = test_data[\"comment_text\"]\n",
        "\n",
        "bad_comment_cat = ['toxic', 'severe_toxic', 'obscene', 'threat',\\\n",
        "       'insult', 'identity_hate']\n",
        "Y_train = train_data[bad_comment_cat]\n",
        "This code sets up a Keras tokenizer to process text data, and extracts comment text from training and testing data for a natural language processing task.\n",
        "\n",
        "tokenizer is a Keras tokenizer object that is used to preprocess the text data. It is initialized with several parameters:\n",
        "num_words is the maximum number of words to keep, based on word frequency. Any less frequent words will be discarded.\n",
        "filters is a string of characters to remove from the text data. Here, it removes punctuation marks and numbers.\n",
        "lower is a Boolean parameter indicating whether to convert all text to lowercase.\n",
        "split is a string indicating the separator to use when splitting the text into words. Here, it is set to split on whitespace.\n",
        "X_train_raw and X_test_raw are Pandas Series objects that contain the comment text data from the training and testing sets, respectively.\n",
        "\n",
        "bad_comment_cat is a list of labels for the six categories of bad comments that the model will try to classify: toxic, severe_toxic, obscene, threat, insult, and identity_hate.\n",
        "\n",
        "Y_train is a Pandas DataFrame object that contains the labels for the six categories of bad comments in the training set. It is extracted from the train_data DataFrame using the bad_comment_cat labels."
      ],
      "metadata": {
        "id": "3tqVn-wyCN7M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4sHeduiti6k",
        "outputId": "5b7a1d67-2ae5-4d95-f449-e1571f923460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(143613, 120)   (143613, 6)   (15958, 120)   (15958, 6)\n"
          ]
        }
      ],
      "source": [
        "# preprocess data\n",
        "X_train_raw = X_train_raw.apply(lambda x: preprocess(str(x)))\n",
        "X_test_raw = X_test_raw.apply(lambda x: preprocess(str(x)))\n",
        "# example X_train_raw.loc[126]\n",
        "\n",
        "# tokenize comment text\n",
        "tokenizer.fit_on_texts(X_train_raw)\n",
        "tokenizer.fit_on_texts(X_test_raw)\n",
        "\n",
        "X_train = pad_sequences(tokenizer.texts_to_sequences(X_train_raw),\\\n",
        "                        maxlen = max_len, truncating = \"pre\")\n",
        "X_test = pad_sequences(tokenizer.texts_to_sequences(X_test_raw),\\\n",
        "                       maxlen = max_len, truncating = \"pre\")\n",
        "\n",
        "# shuffle training data and split it into a training part and a validation part\n",
        "x_train, x_val, y_train, y_val = train_test_split(X_train,Y_train,train_size=0.9, random_state=199)\n",
        "print(x_train.shape,\" \",y_train.shape,\" \",x_val.shape,\" \",y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this code preprocesses the text data by applying the preprocess() function defined earlier, tokenizes the comment text using the tokenizer object, pads the tokenized sequences to a fixed length of max_len, and splits the training data into a training set and a validation set.\n",
        "\n",
        "X_train_raw and X_test_raw are preprocessed using the preprocess() function defined earlier. The apply() method applies the function to each element of the Pandas Series object.\n",
        "\n",
        "tokenizer.fit_on_texts() is called on both the preprocessed X_train_raw and X_test_raw Series objects. This builds the vocabulary index based on word frequency.\n",
        "\n",
        "pad_sequences() is called on the tokenized sequences of X_train_raw and X_test_raw using the tokenizer object. This pads the sequences to a fixed length of max_len and truncates sequences that exceed that length.\n",
        "\n",
        "train_test_split() is called on the padded training data X_train and labels Y_train to split it into a training set (x_train and y_train) and a validation set (x_val and y_val) for model training.\n",
        "\n",
        "The dimensions of the training and validation sets are printed to confirm the correct shape.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ltN5ltdaCeLG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Womf87thti6l"
      },
      "source": [
        "# RNN models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gz6fxkFrti6l"
      },
      "outputs": [],
      "source": [
        "def get_weights(embedding_vectors,embedding_dim):\n",
        "    global num_tokens,tokenizer\n",
        "\n",
        "    # assign vectors to words using the pretrained model embedding_vectors\n",
        "    embedding_weights = np.zeros((num_tokens,embedding_dim))\n",
        "\n",
        "    # count how many words are not assigned with the pretrained model.\n",
        "    # By default, vectors associated to words are zero vectors.\n",
        "    misses = 0\n",
        "\n",
        "    # the index in word_index starts with 1\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        vector = embedding_vectors.get(word)\n",
        "        # the word_index is ordered by word frequency\n",
        "        if i>=num_tokens :\n",
        "            break\n",
        "        elif vector is not None:\n",
        "            embedding_weights[i] = vector\n",
        "        else:\n",
        "            if len(word)<20:\n",
        "                word = Word(word)\n",
        "                word = word.spellcheck()[0][0]\n",
        "                vector = embedding_vectors.get(str(word))\n",
        "                if vector is not None:\n",
        "                    embedding_weights[i] = vector\n",
        "                else:\n",
        "                    misses +=1\n",
        "                    #print(word)\n",
        "            else:\n",
        "                misses +=1\n",
        "                #print(word)\n",
        "\n",
        "    print(f\"The number of missed words is {misses}\")\n",
        "\n",
        "    return embedding_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function is used to create the weight matrix for the embedding layer of a neural network model. The weight matrix is used to initialize the weights of the embedding layer with pre-trained word embeddings.\n",
        "\n",
        "The function takes two inputs:\n",
        "\n",
        "embedding_vectors: a dictionary mapping words to their corresponding pre-trained embedding vectors\n",
        "embedding_dim: the dimensionality of the embedding vectors\n",
        "The function first initializes an all-zero matrix with shape (num_tokens, embedding_dim), where num_tokens is the number of tokens in the tokenizer plus one (to account for the 0 padding token). For each word in the tokenizer's word index, the function checks if the word is in the embedding_vectors dictionary. If it is, the corresponding embedding vector is assigned to the corresponding row of the weight matrix. If it is not, the function attempts to correct the spelling of the word using the Word class from the textblob library. If the corrected word is in the embedding_vectors dictionary, the corresponding embedding vector is assigned to the corresponding row of the weight matrix. If neither the original nor the corrected word is in the embedding_vectors dictionary, the corresponding row of the weight matrix is left as zeros.\n",
        "\n",
        "The function returns the weight matrix. If any words in the tokenizer's word index were not found in the embedding_vectors dictionary, the function prints the number of missed words.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CIubnl_5DALa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCBw2Xz06hSn",
        "outputId": "a7e7f78b-5ca6-4efd-88c4-f91358a18061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-05 14:55:41--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.157.162.108, 108.157.162.35, 108.157.162.83, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.157.162.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1523785255 (1.4G) [application/zip]\n",
            "Saving to: ‘crawl-300d-2M.vec.zip’\n",
            "\n",
            "crawl-300d-2M.vec.z 100%[===================>]   1.42G   136MB/s    in 12s     \n",
            "\n",
            "2023-04-05 14:55:53 (117 MB/s) - ‘crawl-300d-2M.vec.zip’ saved [1523785255/1523785255]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJgjK9Xo6s-X",
        "outputId": "0543692a-00e1-4dab-b43a-264dede7d8e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  crawl-300d-2M.vec.zip\n",
            "  inflating: crawl-300d-2M.vec       \n",
            "crawl-300d-2M.vec  crawl-300d-2M.vec.zip  drive  sample_data\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!unzip crawl*.zip\n",
        "!ls\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgDhhkI5ti6m"
      },
      "outputs": [],
      "source": [
        "# read the pretrained model fastText\n",
        "embedding_vectors_fasttext = {}\n",
        "with open(\"crawl-300d-2M.vec\",\"r\") as file:\n",
        "    file.readline()\n",
        "    for line in file:\n",
        "        word , vector = line.split(maxsplit=1)\n",
        "        vector = np.fromstring(vector,\"float32\",sep=\" \")\n",
        "        embedding_vectors_fasttext[word] = vector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code reads a file named \"crawl-300d-2M.vec\" containing pretrained word embeddings using the fastText model. The file is read line by line and each line contains a word followed by its vector representation. The first line of the file is skipped since it contains metadata. The word and its corresponding vector are stored in a dictionary named embedding_vectors_fasttext."
      ],
      "metadata": {
        "id": "6CyrhGPRDUga"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgUbxylXti6n"
      },
      "outputs": [],
      "source": [
        "# assign vectors to words using the pretrained model fasttext\n",
        "embedding_weights_fasttext = get_weights(embedding_vectors_fasttext,embedding_dim=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuprhO0C_ISt"
      },
      "outputs": [],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVEy79S7_KWD"
      },
      "outputs": [],
      "source": [
        "!unzip glove*.zip\n",
        "!ls\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiZe8Mv7ti6o"
      },
      "outputs": [],
      "source": [
        "# read the pretrained model GloVe\n",
        "embedding_vectors_glove = {}\n",
        "with open(\"glove.6B.300d.txt\",\"r\") as file:\n",
        "    for line in file:\n",
        "        word , vector = line.split(maxsplit=1)\n",
        "        vector = np.fromstring(vector,\"float32\",sep=\" \")\n",
        "        embedding_vectors_glove[word] = vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyqgkMJAti6p"
      },
      "outputs": [],
      "source": [
        "# assign vectors to words using the pretrained model GloVe\n",
        "embedding_weights_glove = get_weights(embedding_vectors_glove,embedding_dim=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above assigns pre-trained word embeddings to words in the vocabulary using the FastText model. The get_weights function takes in two arguments, embedding_vectors and embedding_dim, where embedding_vectors is a dictionary containing the pre-trained word embeddings and embedding_dim is an integer specifying the dimensionality of the embeddings.\n",
        "\n",
        "The function first initializes a zero matrix of shape (num_tokens, embedding_dim), where num_tokens is the vocabulary size plus one (including 0) and embedding_dim is the specified dimensionality. It then iterates over the words in the tokenizer's word_index, which is a dictionary mapping each word to its integer index, and assigns the corresponding pre-trained embedding vector if available in the embedding_vectors dictionary. If a pre-trained vector is not available for a given word, the function attempts to find a spelling correction for the word using the Word class from the textblob library and assigns the corresponding pre-trained embedding vector if available. If a pre-trained vector is still not available for a given word, the function keeps the corresponding row in the embedding_weights matrix as a zero vector.\n",
        "\n",
        "The function returns the embedding_weights matrix, which contains the assigned pre-trained embedding vectors for the words in the vocabulary. In this case, the pre-trained embeddings are from the FastText model, and the resulting embedding_weights_fasttext matrix contains the assigned embeddings."
      ],
      "metadata": {
        "id": "LR4CpyJ5EIsS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2LXVQS8ti6p"
      },
      "outputs": [],
      "source": [
        "def GRU_model_glove():\n",
        "    global max_len,num_tokens,embedding_weights_glove\n",
        "\n",
        "    inputs = layers.Input(shape=(max_len,))\n",
        "\n",
        "    x = layers.Embedding(input_dim=num_tokens,\\\n",
        "                         output_dim=embedding_dim,\\\n",
        "                         embeddings_initializer=keras.initializers.Constant(embedding_weights_glove),\\\n",
        "                         trainable=True)(inputs)\n",
        "\n",
        "    x = layers.SpatialDropout1D(0.3)(x)\n",
        "\n",
        "    forward_layer = layers.GRU(42,return_sequences=True)\n",
        "    backward_layer = layers.GRU(42,activation=\"relu\",dropout=0.1,return_sequences=True,go_backwards=True)\n",
        "    x = layers.Bidirectional(forward_layer,backward_layer=backward_layer)(x)\n",
        "\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "    outputs = layers.Dense(units=6,activation='sigmoid')(x)\n",
        "\n",
        "    model = keras.models.Model(inputs=inputs, outputs=outputs, name=\"GRU_model_glove\")\n",
        "\n",
        "    model.compile(optimizer=tf.optimizers.Adam(),\\\n",
        "                  loss=tf.losses.BinaryCrossentropy(),\\\n",
        "                  metrics=['AUC'])\n",
        "\n",
        "    return model\n",
        "\n",
        "GRU_model_glove = GRU_model_glove()\n",
        "GRU_model_glove.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a Keras model definition for a GRU model with GloVe embeddings, which can be used to classify comments into 6 categories. Here are the details of the model:\n",
        "\n",
        "The input layer takes sequences of integers of length max_len as input.\n",
        "An embedding layer is added on top of the input layer. The embedding layer uses pre-trained GloVe embeddings. The embedding layer is trainable.\n",
        "A dropout layer with a dropout rate of 0.3 is added on top of the embedding layer to prevent overfitting.\n",
        "A bidirectional GRU layer with 42 units is added on top of the dropout layer. The output sequences of the GRU layer are returned.\n",
        "A global max pooling layer is added on top of the GRU layer. The purpose of the global max pooling layer is to extract the most important features from the GRU output sequences.\n",
        "A dense layer with 6 units and a sigmoid activation function is added on top of the global max pooling layer. The dense layer outputs the predicted probabilities for each of the 6 categories.\n",
        "The model is compiled with the Adam optimizer, binary crossentropy loss function, and AUC metric.\n",
        "The model summary shows the number of trainable parameters for each layer and the total number of trainable parameters.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MTt5iA91FFIk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4P2iU9SLGDC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yMqaCwMti6q"
      },
      "outputs": [],
      "source": [
        "history = GRU_model_glove.fit(x_train, y_train, epochs=2,\\\n",
        "                              batch_size=32, validation_data=(x_val,y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is training a GRU (Gated Recurrent Unit) model using GloVe (Global Vectors for Word Representation) embeddings on a dataset with a training set x_train and corresponding labels y_train, and a validation set x_val and corresponding labels y_val.\n",
        "\n",
        "The fit method is used to train the model on the training data. Here are the arguments used in this method:\n",
        "\n",
        "x_train: This is the input data for the training set. It should be a numpy array or a list of numpy arrays, depending on the model's input shape.\n",
        "\n",
        "y_train: This is the corresponding target labels for the training set. It should also be a numpy array or a list of numpy arrays, depending on the model's output shape.\n",
        "\n",
        "epochs: This is the number of times the model will iterate over the entire training set. In this code snippet, the model will train for 2 epochs.\n",
        "\n",
        "batch_size: This is the number of samples that will be processed in each training batch. The default batch size is 32, which is also the value used in this code snippet.\n",
        "\n",
        "validation_data: This is the validation data used to evaluate the model's performance during training. It should also be a tuple of input data and corresponding target labels.\n",
        "\n",
        "The fit method returns a history object, which contains information about the model's training history. This object is assigned to the variable history.\n",
        "\n",
        "So, the code you provided is training a GRU model using GloVe embeddings on the given data for two epochs, with a batch size of 32 and using a validation set to evaluate the model's performance. The training history is then stored in the history object.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iuXMPHOyGEF4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHVOu22JmlyL"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADHMzsT-mlyL"
      },
      "outputs": [],
      "source": [
        "prediction = GRU_model_glove.predict(x_val)\n",
        "y_pred = (prediction > 0.5)\n",
        "print(\"Accuracy of the model : \", accuracy_score(y_pred, y_val))\n",
        "print('F1-score: ', f1_score(y_pred, y_val,average='weighted'))\n",
        "print(\"Precision Score : \",precision_score(y_val, y_pred,  average='micro'))\n",
        "print(\"Recall Score : \",recall_score(y_val, y_pred, average='micro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSeoIKZVmlyO"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "target_names = ['Toxic', 'severe_toxic', 'obscene','threat','insult','identity hate']\n",
        "print(classification_report(y_val, y_pred, target_names=target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CjWVNQ3ti6r"
      },
      "outputs": [],
      "source": [
        "def GRU_model_fasttext():\n",
        "    global max_len,num_tokens,embedding_weights_fasttext\n",
        "\n",
        "    inputs = layers.Input(shape=(max_len,))\n",
        "\n",
        "    x = layers.Embedding(input_dim=num_tokens,\\\n",
        "                         output_dim=embedding_dim,\\\n",
        "                         embeddings_initializer=keras.initializers.Constant(embedding_weights_fasttext),\\\n",
        "                         trainable=False)(inputs)\n",
        "\n",
        "    x = layers.SpatialDropout1D(0.3)(x)\n",
        "\n",
        "    forward_layer = layers.GRU(64,return_sequences=True)\n",
        "    backward_layer = layers.GRU(64,activation=\"relu\",dropout=0.3,return_sequences=True,go_backwards=True)\n",
        "    x = layers.Bidirectional(forward_layer,backward_layer=backward_layer)(x)\n",
        "\n",
        "    avg_pool = layers.GlobalAveragePooling1D()(x)\n",
        "    max_pool = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.concatenate([avg_pool,max_pool])\n",
        "\n",
        "    outputs = layers.Dense(units=6,activation='sigmoid')(x)\n",
        "\n",
        "    model = keras.models.Model(inputs=inputs, outputs=outputs, name=\"GRU_model\")\n",
        "\n",
        "    model.compile(optimizer=tf.optimizers.Adam(),\\\n",
        "                  loss=tf.losses.BinaryCrossentropy(),\\\n",
        "                  metrics=['AUC'])\n",
        "\n",
        "    return model\n",
        "\n",
        "GRU_model_fasttext = GRU_model_fasttext()\n",
        "GRU_model_fasttext.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdjYp5bn_Ars"
      },
      "outputs": [],
      "source": [
        "history1 = GRU_model_fasttext.fit(x_train, y_train, \\\n",
        "                                 epochs=2, batch_size=32,\\\n",
        "                                 validation_data=(x_val,y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vk5-cIWdmlyO"
      },
      "outputs": [],
      "source": [
        "prediction2 = GRU_model_fasttext.predict(x_val)\n",
        "y_pred = (prediction2 > 0.5)\n",
        "print(\"Accuracy of the model : \", accuracy_score(y_pred, y_val))\n",
        "print('F1-score: ', f1_score(y_pred, y_val,average='weighted'))\n",
        "print(\"Precision Score : \",precision_score(y_val, y_pred,  average='micro'))\n",
        "print(\"Recall Score : \",recall_score(y_val, y_pred, average='micro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8wpH1vAmlyP"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "target_names = ['Toxic', 'severe_toxic', 'obscene','threat','insult','identity hate']\n",
        "print(classification_report(y_val, y_pred, target_names=target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dwc5kQ8ti6s"
      },
      "source": [
        "# Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po-fh5TVti6t"
      },
      "outputs": [],
      "source": [
        "model_nums = 2\n",
        "size1 = x_train.shape[0]\n",
        "\n",
        "y_train_pred = np.zeros((model_nums,size1,6),dtype=\"float32\")\n",
        "y_train_pred[0] = GRU_model_fasttext.predict(x_train)\n",
        "y_train_pred[1] = GRU_model_glove.predict(x_train)\n",
        "\n",
        "size2 = X_test.shape[0]\n",
        "y_test_pred = np.zeros((model_nums,size2,6),dtype=\"float32\")\n",
        "y_test_pred[0] = GRU_model_fasttext.predict(X_test)\n",
        "y_test_pred[1] = GRU_model_glove.predict(X_test)\n",
        "\n",
        "y_pred = np.zeros((size2,6),dtype=\"float32\")\n",
        "\n",
        "for i in range(6):\n",
        "    lg = LogisticRegression()\n",
        "    temp = np.zeros((size1,model_nums),dtype=\"float32\")\n",
        "    for j in range(model_nums):\n",
        "        temp[:,j] = y_train_pred[j,:,i]\n",
        "    lg.fit(temp,y_train[bad_comment_cat[i]])\n",
        "\n",
        "    temp = np.zeros((size2,model_nums),dtype=\"float32\")\n",
        "    for j in range(model_nums):\n",
        "        temp[:,j] = y_test_pred[j,:,i]\n",
        "    y_pred[:,i] = lg.predict_proba(temp)[:,1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR_5Fceiti6t"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5Bf5sG_ti6u"
      },
      "outputs": [],
      "source": [
        "submission_result[bad_comment_cat] = y_pred\n",
        "submission_result.to_csv(\"submission.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1zFIy1YpVWX"
      },
      "outputs": [],
      "source": [
        "submission = pd.read_csv('submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wOVTWNQpG2W"
      },
      "outputs": [],
      "source": [
        "submission.head(10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "papermill": {
      "duration": 3015.186557,
      "end_time": "2021-01-22T16:14:41.015227",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-01-22T15:24:25.828670",
      "version": "2.1.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}